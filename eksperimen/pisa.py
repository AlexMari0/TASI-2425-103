# -*- coding: utf-8 -*-
"""fix_bgt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AAlGyafnGZyQGA4msrjesGQ0ibQfEBtv
"""

# ============================
# Import Libraries
# ============================
import json
import os
import torch
import nltk
import pandas as pd
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BertTokenizer, BertModel
from sentence_transformers import SentenceTransformer, util

import numpy as np
from typing import List

# ============================
# Configurations and Directories
# ============================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Base output directory
BASE_DIR = "output"

# Define output subdirectories
QA_Pairs_DIR = os.path.join(BASE_DIR, "QA_Pairs")
BERTScore_DIR = os.path.join(BASE_DIR, "BERTScore")
Cosine_Similarity_DIR = os.path.join(BASE_DIR, "Cosine_Similarity")
Weight_DIR = os.path.join(BASE_DIR, "weight")
QA_Vector_DIR = os.path.join(BASE_DIR, "QA_Vectors")

# Ensure all output directories exist
os.makedirs(QA_Pairs_DIR, exist_ok=True)
os.makedirs(BERTScore_DIR, exist_ok=True)
os.makedirs(Cosine_Similarity_DIR, exist_ok=True)
os.makedirs(Weight_DIR, exist_ok=True)
os.makedirs(QA_Vector_DIR, exist_ok=True)

# Path to input data
DATA_PATH = "pisatest.json"  # Path to input dataset

# Download NLTK tokenizer
nltk.download('punkt')
nltk.download('punkt_tab')

# ============================
# Load Data
# ============================
def load_data():
    df = pd.read_json(DATA_PATH)
    contexts = df['context'].dropna().tolist()  # Select first 5 rows
    titles = df['title'].dropna().tolist()  # Select first 5 rows
    return contexts, titles

# Load data
contexts, titles = load_data()

"""# GENERATE QA"""

# ============================
# QA Generation Class
# ============================
class QuestionGenerator:
    def __init__(self, model_name: str):
        self.ANSWER_TOKEN = "<answer>"
        self.CONTEXT_TOKEN = "<context>"
        self.SEQ_LENGTH = 512
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(DEVICE)
        self.model.eval()

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

    def generate_from_sentences(self, context: str) -> List[dict]:
        sentences = nltk.sent_tokenize(context)
        qa_pairs = []

        for i in range(0, len(sentences), 8):
            batch = sentences[i:i + 8]
            batch_input = [f"{self.ANSWER_TOKEN} {s} {self.CONTEXT_TOKEN} {context}" for s in batch]

            inputs = self.tokenizer(batch_input, padding=True, truncation=True, max_length=self.SEQ_LENGTH, return_tensors="pt").to(DEVICE)
            with torch.no_grad():
                outputs = self.model.generate(**inputs, max_length=64, num_beams=1, early_stopping=True, pad_token_id=self.tokenizer.pad_token_id)

            for output, sentence in zip(outputs, batch):
                question = self.tokenizer.decode(output, skip_special_tokens=True)
                qa_pairs.append({"question": question, "answer": sentence})  # Only include question and answer

        return qa_pairs

# ============================
# Generate all QA Pairs
# ============================
def generate_all_qa_pairs(qg_model, contexts: List[str], titles: List[str], model_name: str) -> List[dict]:
    results = []
    for context, title in tqdm(zip(contexts, titles), total=len(contexts), desc=f"Generating QA with {model_name}"):
        qa_pairs = qg_model.generate_from_sentences(context)
        results.append({"title": title, "context": context, "qa_pairs": qa_pairs})  # Add title and context once
    return results

def save_qa_output(results: List[dict], filename: str):
    path = os.path.join(QA_Pairs_DIR, filename)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

# ============================
# Generate QA Pairs for each model
# ============================
qa_pairs_bert = []  # Store BERT QA pairs
qa_pairs_t5 = []    # Store T5 QA pairs
qa_pairs_pegasusx = []  # Store PEGASUSX QA pairs

all_models = [
    ("/home/samuel/tasi2425103/revisi/model/final_bert_2/", "BERT", "bert_qa_results_sync.json"),
    ("/home/samuel/tasi2425103/revisi/model/final_t5/", "T5", "t5_qa_results_sync.json"),
    ("/home/samuel/tasi2425103/revisi/model/final_pegasusx/", "PEGASUSX", "pegasusx_qa_results_sync.json")
]

qa_pairs_all = {}
for model_path, model_name, output_file in all_models:
    print(f"\nGenerating QA pairs using {model_name}")
    qg = QuestionGenerator(model_path)
    qa_pairs = generate_all_qa_pairs(qg, contexts, titles, model_name)

    # Store QA pairs in respective lists
    if model_name == "BERT":
        qa_pairs_bert = qa_pairs
    elif model_name == "T5":
        qa_pairs_t5 = qa_pairs
    elif model_name == "PEGASUSX":
        qa_pairs_pegasusx = qa_pairs

    save_qa_output(qa_pairs, output_file)
    qa_pairs_all[model_name] = qa_pairs

# Fungsi menghitung total QA pairs (jumlah Q&A di dalam "qa_pairs")
def count_total_questions_answers(qa_data):
    total = 0
    for item in qa_data:
        total += len(item.get("qa_pairs", []))
    return total

# ============================
# Hitung dan tampilkan total QA (Q&A) per model
# ============================
bert_total = count_total_questions_answers(qa_pairs_bert)
t5_total = count_total_questions_answers(qa_pairs_t5)
pegasusx_total = count_total_questions_answers(qa_pairs_pegasusx)
total_all_qa = bert_total + t5_total + pegasusx_total

print("\n====== QA Pairs Summary (Based on Q&A count) ======")
print(f"Total Q&A pairs (BERT): {bert_total}")
print(f"Total Q&A pairs (T5): {t5_total}")
print(f"Total Q&A pairs (PEGASUSX): {pegasusx_total}")
print(f"Total Q&A pairs (All Models Combined): {total_all_qa}")

print("\n‚úî All QA pairs generated successfully!")

"""# BERTSCORE"""



# =========================================================
# 1. Impor pustaka
# =========================================================
import os, json, numpy as np, torch
from bert_score import BERTScorer
from tqdm.auto import tqdm                # ‚Üê progress bar

# =========================================================
# 2. Konfigurasi
# =========================================================
MODEL_NAME = "bert-base-multilingual-cased"
DEVICE     = "cuda" if torch.cuda.is_available() else "cpu"

OUTPUT_DIR    = "output"
BERTSCORE_DIR = os.path.join(OUTPUT_DIR, "BERTScore")
os.makedirs(BERTSCORE_DIR, exist_ok=True)

# =========================================================
# 3. Inisialisasi BERTScorer sekali saja
# =========================================================
scorer = BERTScorer(
    model_type=MODEL_NAME,
    device=DEVICE,
    lang="id",
    rescale_with_baseline=False
)

# =========================================================
# 4. Fungsi inti
# =========================================================
def compute_bertscore_single_context(qa_pairs, context):
    candidates = [f"{qa['question']} {qa['answer']}" for qa in qa_pairs]
    references = [context] * len(qa_pairs)
    P, R, F1   = scorer.score(candidates, references)
    return list(zip(P.tolist(), R.tolist(), F1.tolist()))  # tuples of floats

def process_qa_files(file_paths):
    results = {}

    # tqdm untuk daftar berkas
    for fp in tqdm(file_paths, desc="üìÇ Memroses file", dynamic_ncols=True):
        with open(fp) as f:
            dataset = json.load(f)

        model_key = os.path.basename(fp).split("_")[0]  # 'bert', 't5', ‚Ä¶
        results.setdefault(model_key,
            {"data": [], "precision": [], "recall": [], "f1": []})

        # tqdm untuk tiap item dalam berkas
        for item in tqdm(dataset, desc=f"  ‚Üí {model_key}", leave=False, dynamic_ncols=True):
            brs = compute_bertscore_single_context(item["qa_pairs"], item["context"])

            # simpan
            results[model_key]["data"].append({
                "title"    : item["title"],
                "context"  : item["context"],
                "bertscore": [list(t) for t in brs]    # tuple ‚Üí list utk JSON
            })

            P, R, F1 = zip(*brs)
            results[model_key]["precision"].extend(P)
            results[model_key]["recall"].extend(R)
            results[model_key]["f1"].extend(F1)

    return results

# =========================================================
# 5. Jalankan & simpan
# =========================================================
qa_files = [
    "output/QA_Pairs/bert_qa_results_sync.json",
    "output/QA_Pairs/pegasusx_qa_results_sync.json",
    "output/QA_Pairs/t5_qa_results_sync.json",
]

all_results = process_qa_files(qa_files)

# ‚îÄ‚îÄ simpan skor per-model
for m, res in all_results.items():
    out = os.path.join(BERTSCORE_DIR, f"{m}_bertscore_results.json")
    with open(out, "w") as f:
        json.dump(res["data"], f, indent=4, ensure_ascii=False)
    print(f"‚úîÔ∏è  {m}: skor tersimpan ‚Üí {out}")

# =========================================================
# 6. Hitung & simpan bobot model
# =========================================================
total_mean_f1 = sum(np.mean(r["f1"]) for r in all_results.values())
weights = {m: float(np.mean(r["f1"]) / total_mean_f1) for m, r in all_results.items()}

weights_path = os.path.join(BERTSCORE_DIR, "model_weights.txt")
with open(weights_path, "w") as f:
    for m, w in weights.items():
        f.write(f"Model: {m}, Weight: {w:.6f}\n")

print("\nBobot model:")
for m, w in weights.items():
    print(f"  ‚Ä¢ {m:<10}: {w:.6f}")
print(f"\nüìÑ  File bobot tersimpan di {weights_path}")

# =========================================================
# 7. Simpan Statistik Agregat BERTScore
# =========================================================
stats_output = {}

for model_name, model_results in all_results.items():          # ‚Üê pakai all_results
    stats_output[model_name] = {
        "precision": {
            "avg": float(np.mean(model_results["precision"])),
            "min": float(np.min(model_results["precision"])),
            "max": float(np.max(model_results["precision"]))
        },
        "recall": {
            "avg": float(np.mean(model_results["recall"])),
            "min": float(np.min(model_results["recall"])),
            "max": float(np.max(model_results["recall"]))
        },
        "f1": {
            "avg": float(np.mean(model_results["f1"])),
            "min": float(np.min(model_results["f1"])),
            "max": float(np.max(model_results["f1"]))
        }
    }

# ‚îÄ‚îÄ Simpan ke JSON
stats_file = os.path.join(BERTSCORE_DIR, "stats_bertscore.json")
with open(stats_file, "w", encoding="utf-8") as f:
    json.dump(stats_output, f, ensure_ascii=False, indent=2)

print(f"üìä Statistik BERTScore disimpan ke: {stats_file}")

"""# COSINE SIMILARITY"""

import os, json, pandas as pd, numpy as np
from tqdm import tqdm
from sentence_transformers import SentenceTransformer, util

# ---------- Konfigurasi ----------
BASE_DIR               = "output"
QA_PAIRS_DIR           = os.path.join(BASE_DIR, "QA_Pairs")
COSINE_SIMILARITY_DIR  = os.path.join(BASE_DIR, "Cosine_Similarity")
os.makedirs(COSINE_SIMILARITY_DIR, exist_ok=True)

MODEL_FILES = {
    "BERT"      : "bert_qa_results_sync.json",
    "T5"        : "t5_qa_results_sync.json",
    "PEGASUSX"  : "pegasusx_qa_results_sync.json",
}

embedder = SentenceTransformer("all-MiniLM-L6-v2")

def load_qa_pairs(path: str):
    with open(path, encoding="utf-8") as f:
        return json.load(f)

def calculate_cosine_similarity_for_qa_pairs(qa_pairs_all, model_name: str):
    cosine_results   = []
    all_similarities = []
    total_qa_count   = 0

    for item in tqdm(qa_pairs_all, desc=f"[{model_name}] Cosine Similarity"):
        title     = item["title"]
        questions = [q["question"] for q in item["qa_pairs"]]

        if len(questions) < 2:
            total_qa_count += len(questions)
            continue

        total_qa_count += len(questions)
        embs          = embedder.encode(questions, convert_to_tensor=True)
        cosine_matrix = util.cos_sim(embs, embs).cpu().numpy()

        # Ambil hanya elemen di luar diagonal
        mask = ~np.eye(len(questions), dtype=bool)
        all_similarities.extend(cosine_matrix[mask])

        cosine_results.append({
            "title"         : title,
            "model"         : model_name,
            "questions"     : questions,
            "cosine_matrix" : cosine_matrix.round(4).tolist(),
            "labels"        : [f"QA{i+1}" for i in range(len(questions))]
        })

    sims_arr = np.array(all_similarities)
    stats = {
        "model"                   : model_name,
        "total_contexts"         : len(cosine_results),
        "total_qa_pairs"         : total_qa_count,
        "mean_cosine_similarity" : round(float(sims_arr.mean()), 4) if sims_arr.size else 0,
        "std_cosine_similarity"  : round(float(sims_arr.std()), 4)  if sims_arr.size else 0,
        "min_cosine_similarity"  : round(float(sims_arr.min()), 4)  if sims_arr.size else 0,
        "max_cosine_similarity"  : round(float(sims_arr.max()), 4)  if sims_arr.size else 0,
    }
    return cosine_results, stats



def main():
    all_stats = {}

    for model_name, filename in MODEL_FILES.items():
        file_path = os.path.join(QA_PAIRS_DIR, filename)
        qa_pairs  = load_qa_pairs(file_path)

        cosine_results, stats = calculate_cosine_similarity_for_qa_pairs(qa_pairs, model_name)

        # Simpan hasil cosine matrix
        out_path = os.path.join(COSINE_SIMILARITY_DIR, f"{model_name}_qa_cosine_similarity.json")
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(cosine_results, f, indent=4, ensure_ascii=False)

        # Simpan statistik
        all_stats[model_name] = stats

    # Simpan file statistik
    stats_path = os.path.join(COSINE_SIMILARITY_DIR, "cosine_similarity_stats.json")
    with open(stats_path, "w", encoding="utf-8") as f:
        json.dump(all_stats, f, indent=4, ensure_ascii=False)

    # ---------- Ringkasan ----------
    print("\n‚úÖ Cosine similarity untuk semua model telah dihitung & disimpan.")
    for m, stats in all_stats.items():
        print(f"\nüìä {m} Stats:")
        for k, v in stats.items():
            print(f" - {k}: {v}")

if __name__ == "__main__":
    main()

"""# ENCODE VECTOR"""

import json
import os
import torch
from transformers import BertTokenizer, BertModel

# Set device
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Directory to save the vectors
QA_Vector_DIR = "output/QA_Vectors"
os.makedirs(QA_Vector_DIR, exist_ok=True)

# ========== LOAD QA DATA ==========

def load_qa_data(file_path):
    """Load QA pairs from JSON file."""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

# Load data
qa_pairs_bert = load_qa_data("output/QA_Pairs/bert_qa_results_sync.json")
qa_pairs_t5 = load_qa_data("output/QA_Pairs/t5_qa_results_sync.json")
qa_pairs_pegasusx = load_qa_data("output/QA_Pairs/pegasusx_qa_results_sync.json")

# ========== GROUP BY CONTEXT ==========

def group_by_context(data):
    """Group QA pairs by unique (title, context) pair."""
    grouped = {}
    for item in data:
        title = item.get("title", "")
        context = item.get("context", "")
        key = (title, context)
        if key not in grouped:
            grouped[key] = []
        grouped[key].extend(item.get("qa_pairs", []))
    return grouped

# Grouped data per model
grouped_bert = group_by_context(qa_pairs_bert)
grouped_t5 = group_by_context(qa_pairs_t5)
grouped_pegasusx = group_by_context(qa_pairs_pegasusx)

# ========== LOAD BERT MODEL & TOKENIZER ==========

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased").to(DEVICE)
model.eval()

# ========== EMBEDDING FUNCTION ==========

def get_cls_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
    cls_embedding = outputs.last_hidden_state[:, 0, :]
    return cls_embedding.squeeze().cpu().numpy()

# ========== PROCESS & SAVE TO JSON ==========

def process_and_save_to_json(grouped_data, model_label, output_path):
    result = []
    for (title, context), qa_list in grouped_data.items():
        item_entry = {
            "title": title,
            "context": context,
            "qa_pairs": []
        }
        for qa in qa_list:
            question = qa["question"]
            answer = qa["answer"]

            q_vec = get_cls_embedding(question)
            a_vec = get_cls_embedding(answer)

            item_entry["qa_pairs"].append({
                "question": question,
                "answer": answer,
                "q_vec": q_vec.tolist(),
                "a_vec": a_vec.tolist()
            })
        result.append(item_entry)

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(result, f, indent=4, ensure_ascii=False)

    print(f"‚úÖ Vektor {model_label} berhasil disimpan ke {output_path}")

# ========== RUN ==========

print("üì° Memproses QA pairs dan menyimpan vektor ke file JSON...\n")

# Process and save for each model
process_and_save_to_json(grouped_bert, "BERT", os.path.join(QA_Vector_DIR, "qa_vectors_bert.json"))
process_and_save_to_json(grouped_t5, "T5", os.path.join(QA_Vector_DIR, "qa_vectors_t5.json"))
process_and_save_to_json(grouped_pegasusx, "PEGASUSX", os.path.join(QA_Vector_DIR, "qa_vectors_pegasusx.json"))

print("\n‚úÖ Semua hasil QA vectors lengkap telah disimpan dalam format JSON.")

import json
import os
import torch
from transformers import BertTokenizer, BertModel
from tqdm import tqdm  # Tambahan: untuk progress bar

# ===============================
# SET DEVICE
# ===============================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ===============================
# OUTPUT DIRECTORY
# ===============================
Context_Vector_DIR = "output/Context_Vectors"
os.makedirs(Context_Vector_DIR, exist_ok=True)

# ===============================
# LOAD CONTEXT DATA
# ===============================
def load_context_data(file_path):
    """Load context data from JSON file."""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

# Ubah path sesuai file kamu
context_data = load_context_data("pisatest.json")

# ===============================
# LOAD BERT MODEL & TOKENIZER
# ===============================
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased").to(DEVICE)
model.eval()

# ===============================
# EMBEDDING FUNCTION
# ===============================
def get_cls_embedding(text):
    """Generate the CLS token embedding for the input text."""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
    cls_embedding = outputs.last_hidden_state[:, 0, :]  # CLS token
    return cls_embedding.squeeze().cpu().numpy()

# ===============================
# PROCESS & SAVE
# ===============================
def process_and_save_context_to_json(context_data, output_path):
    result = []
    for item in tqdm(context_data, desc="üîç Memproses konteks"):
        title = item.get("title", "")
        context = item.get("context", "")
        if not context:
            continue  # Lewati jika context kosong

        try:
            context_vec = get_cls_embedding(context)
            result.append({
                "title": title,
                "context": context,
                "context_vec": context_vec.tolist()
            })
        except Exception as e:
            print(f"‚ùå Gagal memproses: {title} ‚Äî {e}")
            continue

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(result, f, indent=2, ensure_ascii=False)

    print(f"\n‚úÖ Vektor konteks berhasil disimpan ke: {output_path}")

# ===============================
# RUN PROCESS
# ===============================
print("üì° Memproses konteks dan menyimpan vektor ke file JSON...\n")
output_file = os.path.join(Context_Vector_DIR, "context_vectors.json")
process_and_save_context_to_json(context_data, output_file)
print("\n‚úÖ Semua hasil vektor konteks telah disimpan.")

import json
import numpy as np
import os

# Fungsi untuk menghitung vektor akhir
def compute_final_vectors(input_path, output_path, weight, model_name):
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    result = []
    for item in data:
        item_entry = {
            "title": item["title"],
            "context": item["context"],
            "qa_pairs_with_final_vectors": [] # New key to store QA pairs with final vectors
        }
        for qa in item.get("qa_pairs", []): # Iterate through the qa_pairs list
            # Pastikan vektor diubah ke numpy array (bukan list)
            q_vec = np.array(qa["q_vec"])
            a_vec = np.array(qa["a_vec"])

            # Hitung vektor akhir dengan menggabungkan q_vec dan a_vec, kemudian mengalikannya dengan bobot
            final_vec = weight * (q_vec + a_vec)

            # Menyimpan hasil dalam format yang diinginkan
            item_entry["qa_pairs_with_final_vectors"].append({
                "question": qa["question"],
                "answer": qa["answer"],
                "final_vector": final_vec.tolist()  # menyimpan vektor dalam bentuk list untuk JSON
            })
        result.append(item_entry)

    # Menyimpan hasil ke file JSON
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=4, ensure_ascii=False)

    print(f"‚úÖ Final vector {model_name} telah disimpan ke: {output_path}")

# ============================
# Proses vektor akhir untuk setiap model
# ============================

def process_final_vectors_for_all_models():
    """
    Fungsi ini memproses dan menyimpan vektor akhir untuk semua model berdasarkan bobot yang dihitung.
    """
    for model_name in model_weights.keys():
        # Path input untuk masing-masing model QA vectors
        input_path = os.path.join(QA_Vector_DIR, f"qa_vectors_{model_name.lower()}.json")
        output_path = os.path.join(QA_Vector_DIR, f"final_vectors_{model_name.lower()}.json")

        # Mengambil bobot model berdasarkan F1 score
        model_weight = model_weights.get(model_name, 0)

        # Hitung dan simpan vektor akhir untuk model ini
        compute_final_vectors(input_path, output_path, model_weight, model_name)

# ============================
# Jalankan perhitungan vektor akhir
# ============================

# Ambil model_weights dari weights (yang dihitung dari skor F1 rata-rata)
model_weights = weights

print("?? Menghitung vektor akhir untuk semua model...\n")
process_final_vectors_for_all_models()
print("\n? Semua vektor akhir untuk semua model telah dihitung dan disimpan.")

import json
import numpy as np
import os
from sklearn.metrics.pairwise import cosine_similarity

"""# WEIGHTED AVERAGING"""

import json
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import os
from collections import defaultdict

# ========================
# Load context vectors
# ========================
with open('output/Context_Vectors/context_vectors.json', 'r', encoding='utf-8') as f:
    context_data = json.load(f)

# Buat lookup: (title, context) ‚ûú context_vec
context_lookup = {
    (item['title'], item['context']): np.array(item['context_vec'])
    for item in context_data
}

# ========================
# Load QA final_vectors dari semua model
# ========================
qa_files = [
    ('BERT', 'output/QA_Vectors/final_vectors_bert.json'),
    ('PEGASUSX', 'output/QA_Vectors/final_vectors_pegasusx.json'),
    ('T5', 'output/QA_Vectors/final_vectors_t5.json')
]

qa_all = []

for model_name, file_path in qa_files:
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
        for item in data:
            key = (item['title'], item.get('context', ''))
            for qa_pair_info in item.get('qa_pairs_with_final_vectors', []):
                qa_all.append({
                    'model': model_name,
                    'title': item['title'],
                    'context': item.get('context', ''),
                    'question': qa_pair_info['question'],
                    'answer': qa_pair_info['answer'],
                    'final_vector': np.array(qa_pair_info['final_vector']),
                    'key': key
                })

# ========================
# Hitung similarity global untuk semua QA
# ========================
qa_with_similarity = []

for qa in qa_all:
    key = qa['key']
    if key in context_lookup:
        context_vec = context_lookup[key]
        similarity = cosine_similarity([qa['final_vector']], [context_vec])[0][0]
        qa_with_similarity.append({
            'title': qa['title'],
            'context': qa['context'],
            'question': qa['question'],
            'answer': qa['answer'],
            'model': qa['model'],
            'similarity': similarity
        })

# ========================
# Ambil QA terbaik berdasarkan similarity global
# ========================
top_k = 89
top_qa_global = sorted(qa_with_similarity, key=lambda x: x['similarity'], reverse=True)[:top_k]

# ========================
# Kelompokkan berdasarkan (title, context)
# ========================
grouped_output = defaultdict(list)

for qa in top_qa_global:
    key = (qa['title'], qa['context'])
    grouped_output[key].append({
        "question": qa["question"],
        "answer": qa["answer"],
        "model": qa["model"],
        "similarity": qa["similarity"]
    })

# Format final untuk simpan ke file
final_output = []
for (title, context), qas in grouped_output.items():
    final_output.append({
        "title": title,
        "context": context,
        "qas": qas
    })

# ========================
# Simpan hasil
# ========================
weighted_qa_dir = os.path.join("output", "Weighted_QA")
os.makedirs(weighted_qa_dir, exist_ok=True)

output_file = os.path.join(weighted_qa_dir, 'weighted_qa_result.json')
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(final_output, f, indent=2, ensure_ascii=False)

print(f"‚úÖ Selesai! Total QA yang disimpan: {top_k} (dalam {len(final_output)} context group)")
print(f"üìÅ Disimpan di: {output_file}")

import os
import json
import numpy as np
import torch
from bert_score import score
from typing import List, Dict

INPUT_PATH  = "output/Weighted_QA/weighted_qa_result.json"

# Define the output directory for weighted QA results
Weighted_QA_DIR = "output/Weighted_QA"
os.makedirs(Weighted_QA_DIR, exist_ok=True)

MODEL_NAME  = "bert-base-multilingual-cased"
BATCH_SIZE  = 16
DEVICE      = "cuda" if torch.cuda.is_available() else "cpu"

def load_qa(path: str) -> List[Dict]:
    with open(path, encoding="utf-8") as f:
        return json.load(f)

def prepare_lists(data: List[Dict]):
    candidates, references = [], []
    for item in data:
        ref  = item["context"].strip()
        # Iterate through the nested 'qas' list
        for qa_pair in item.get("qas", []):
            cand = f"{qa_pair['question'].strip()} {qa_pair['answer'].strip()}"
            candidates.append(cand)
            references.append(ref)
    return candidates, references

def compute_bertscore(
    candidates: List[str], references: List[str],
    model_name: str = MODEL_NAME, batch_size: int = BATCH_SIZE, device: str = DEVICE
):
    P, R, F1 = score(
        cands=candidates,
        refs=references,
        lang="id",
        model_type=model_name,
        batch_size=batch_size,
        device=device,
        verbose=True,
    )
    return P.cpu().numpy().tolist(), R.cpu().numpy().tolist(), F1.cpu().numpy().tolist()

def attach_scores(data: List[Dict], P, R, F1):
    idx = 0
    for item in data:
        for qa in item.get("qas", []): # Iterate through the nested 'qas' list
            qa["bertscore"] = {
                "precision": P[idx],
                "recall": R[idx],
                "f1": F1[idx],
            }
            idx += 1
    return data

def compute_stats(scores: List[float]):
    arr = np.array(scores, dtype=np.float64)
    return {
        "mean":  float(arr.mean()),
        "min":   float(arr.min()),
        "max":   float(arr.max()),
        "std":   float(arr.std(ddof=0)),
    }

def main():
    qa_data = load_qa(INPUT_PATH)
    candidates, references = prepare_lists(qa_data)
    P, R, F1 = compute_bertscore(candidates, references)
    qa_scored = attach_scores(qa_data, P, R, F1)
    out_json = os.path.join(Weighted_QA_DIR, "weighted_qa_result_with_bertscore.json")
    with open(out_json, "w", encoding="utf-8") as f:
        json.dump(qa_scored, f, ensure_ascii=False, indent=2)

    stats = {
        "precision": compute_stats(P),
        "recall":    compute_stats(R),
        "f1":        compute_stats(F1),
        "total_pairs": len(F1),
        "model_name": MODEL_NAME,
    }
    stats_json = os.path.join(Weighted_QA_DIR, "weighted_bertscore_stats.json")
    with open(stats_json, "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    print("==== Ringkasan BERTScore ====")
    for metric in ["precision", "recall", "f1"]:
        s = stats[metric]
        print(f"{metric.capitalize():<10}: "
              f"mean={s['mean']:.4f} | min={s['min']:.4f} | "
              f"max={s['max']:.4f} | std={s['std']:.4f}")
    print(f"\n‚û°  File hasil QA+score   : {out_json}")
    print(f"‚û°  File statistik global : {stats_json}")

if __name__ == "__main__":
    main()

import json, os, numpy as np, torch
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm

# ===========================
# Konfigurasi & Load Model
# ===========================
model_name = "indobenchmark/indobert-base-p1"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
model.eval()

# ===========================
# Pooling untuk Dapatkan Embedding
# ===========================
def mean_pooling(last_hidden, mask):
    mask = mask.unsqueeze(-1).expand(last_hidden.size()).float()
    return (last_hidden * mask).sum(1) / torch.clamp(mask.sum(1), min=1e-9)

def encode(texts, batch_size=16):
    all_embeds = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        enc = tokenizer(batch, padding=True, truncation=True, return_tensors="pt")
        with torch.no_grad():
            out = model(**enc)
        emb = mean_pooling(out.last_hidden_state, enc['attention_mask'])
        all_embeds.append(emb)
    return torch.cat(all_embeds)

def cos_sim(a, b):
    return (a @ b) / (a.norm() * b.norm())

# ===========================
# Load QA Result dari File Weighted
# ===========================
with open("output/Weighted_QA/weighted_qa_result.json", encoding="utf-8") as f:
    data = json.load(f)

# ===========================
# Proses per Konteks
# ===========================
results = []
all_similarity_values = []

for item in tqdm(data, desc="Processing Contexts"):
    context = item["context"]
    qa_pairs = item["qas"]

    if len(qa_pairs) < 2:
        continue  # skip jika hanya ada 1 QA pair

    qa_texts = [qa["question"].replace("answer> ", "").strip() for qa in qa_pairs]
    embeddings = encode(qa_texts)
    num_qa = len(qa_texts)

    similarity_matrix = np.zeros((num_qa, num_qa))
    for i in range(num_qa):
        for j in range(num_qa):
            sim = cos_sim(embeddings[i], embeddings[j]).item()
            similarity_matrix[i][j] = sim
            if i != j:
                all_similarity_values.append(sim)

    results.append({
        "context": context,
        "qa_pairs": qa_pairs,
        "similarity_matrix": similarity_matrix.tolist()
    })

# ===========================
# Simpan Hasil ke Weighted_QA
# ===========================
output_dir = "output/Weighted_QA"
os.makedirs(output_dir, exist_ok=True)

matrix_path = os.path.join(output_dir, "cosine_question_matrix_by_context.json")
with open(matrix_path, "w", encoding="utf-8") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

# ===========================
# Statistik Global
# ===========================
all_similarity_array = np.array(all_similarity_values)
stats = {
    "similarity_mean": float(np.mean(all_similarity_array)),
    "similarity_max": float(np.max(all_similarity_array)),
    "similarity_min": float(np.min(all_similarity_array)),
    "similarity_std": float(np.std(all_similarity_array))
}

stats_path = os.path.join(output_dir, "cosine_question_matrix_stats.json")
with open(stats_path, "w", encoding="utf-8") as f:
    json.dump(stats, f, indent=2)

# ===========================
# Print Ringkasan
# ===========================
print("\n===== Global Cosine Similarity Stats (All Contexts) =====")
for k, v in stats.items():
    print(f"{k}: {v:.4f}")
print(f"\n‚úîÔ∏è Cosine similarity matrix saved to: {matrix_path}")
print(f"‚úîÔ∏è Stats saved to: {stats_path}")

"""# VOTING"""

# ============================
# IMPORT LIBRARIES
# ============================
import os
import json
import numpy as np
from tqdm import tqdm
import torch
from sklearn.metrics.pairwise import cosine_similarity
from transformers import BertTokenizer, BertModel

# ============================
# KONFIGURASI & PATH
# ============================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Path hasil QA masing-masing model
QA_PATHS = {
    "BERT":      "output/QA_Pairs/bert_qa_results_sync.json",
    "T5":        "output/QA_Pairs/t5_qa_results_sync.json",
    "PEGASUSX":  "output/QA_Pairs/pegasusx_qa_results_sync.json"
}

# Path vektor konteks
CONTEXT_VECTOR_PATH = "output/Context_Vectors/context_vectors.json"

# Direktori & nama file output
OUTPUT_DIR = "output/voting"
os.makedirs(OUTPUT_DIR, exist_ok=True)
FINAL_QA_PATH = os.path.join(OUTPUT_DIR, "voting_qa_result.json")

# ============================
# FUNGSI BANTUAN
# ============================
def load_json(file_path):
    """Memuat data dari file JSON."""
    with open(file_path, "r", encoding="utf-8") as f:
        return json.load(f)

def get_cls_embedding(text, tokenizer, model):
    """Membuat embedding [CLS] untuk sebuah teks."""
    inputs = tokenizer(text,
                       return_tensors="pt",
                       truncation=True,
                       padding=True,
                       max_length=128).to(DEVICE)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()

# ============================
# PROSES UTAMA
# ============================
# ============================
# PROSES UTAMA (UPDATED)
# ============================
def main():
    print("\nTahap 1: Memuat model dan data ‚Ä¶")
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    model     = BertModel.from_pretrained("bert-base-uncased").to(DEVICE)
    model.eval()

    print("‚Ä£ Memuat QA hasil tiga model ‚Ä¶")
    bert_data     = load_json(QA_PATHS["BERT"])
    t5_data       = load_json(QA_PATHS["T5"])
    pegasusx_data = load_json(QA_PATHS["PEGASUSX"])

    print("‚Ä£ Memuat vektor konteks ‚Ä¶")
    context_vectors_list = load_json(CONTEXT_VECTOR_PATH)
    context_to_vector = {
        item["context"]: np.array(item["context_vec"])
        for item in context_vectors_list
    }
    print("‚úì Semua aset dimuat.")

    print("\nTahap 2‚Äì3: Menghitung skor dan melakukan voting ‚Ä¶")
    grouped_results = {}  # (title, context) ‚Üí {title, context, qa_pairs}

    for i in tqdm(range(len(bert_data)), desc="Processing Contexts"):
        context_text = bert_data[i]["context"]
        title_text   = bert_data[i].get("title", f"Unknown-{i}")
        context_vector = context_to_vector.get(context_text)

        if context_vector is None:
            continue

        num_qas = len(bert_data[i]["qa_pairs"])

        for j in range(num_qas):
            candidates = {
                "BERT":     bert_data[i]["qa_pairs"][j],
                "T5":       t5_data[i]["qa_pairs"][j],
                "PEGASUSX": pegasusx_data[i]["qa_pairs"][j],
            }

            best_model      = None
            best_similarity = -1.0

            for model_name, qa in candidates.items():
                q_vec = get_cls_embedding(qa["question"], tokenizer, model)
                a_vec = get_cls_embedding(qa["answer"],   tokenizer, model)
                qa_vec = q_vec + a_vec

                sim = cosine_similarity([qa_vec], [context_vector])[0][0]

                if sim > best_similarity:
                    best_similarity = sim
                    best_model      = model_name

            winner_qa = candidates[best_model]

            key = (title_text, context_text)
            if key not in grouped_results:
                grouped_results[key] = {
                    "title": title_text,
                    "context": context_text,
                    "qa_pairs": []
                }

            grouped_results[key]["qa_pairs"].append({
                "question": winner_qa["question"],
                "answer": winner_qa["answer"],
                "winning_model": best_model,
                "similarity_to_context_score": float(best_similarity)
            })

    # Simpan ke file
    print("\nTahap 4: Menyimpan hasil akhir ‚Ä¶")
    final_output = list(grouped_results.values())

    with open(FINAL_QA_PATH, "w", encoding="utf-8") as f:
        json.dump(final_output, f, indent=4, ensure_ascii=False)

    total_qa = sum(len(item["qa_pairs"]) for item in final_output)
    print(f"\n‚úÖ Proses selesai! Hasil voting (berdasarkan context) disimpan di:\n{FINAL_QA_PATH}")
    print(f"Total konteks        : {len(final_output)}")
    print(f"Total QA pairs terpilih: {total_qa}")



if __name__ == "__main__":
    main()

import os
import json
import numpy as np
import torch
from bert_score import score
from typing import List, Dict

# ===============================
# KONFIGURASI
# ===============================
INPUT_PATH = "output/voting/voting_qa_result.json"
OUTPUT_DIR = "output/voting"
os.makedirs(OUTPUT_DIR, exist_ok=True)

MODEL_NAME = "bert-base-multilingual-cased"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ===============================
# FUNGSI UTAMA
# ===============================
def load_qa(path: str) -> List[Dict]:
    with open(path, encoding="utf-8") as f:
        return json.load(f)

def prepare_lists(data: List[Dict]):
    cands, refs = [], []
    for item in data:
        context = item["context"].strip()
        for qa in item["qa_pairs"]:
            cands.append(f"{qa['question'].strip()} {qa['answer'].strip()}")
            refs.append(context)
    return cands, refs

def compute_bertscore(cands, refs):
    P, R, F1 = score(
        cands, refs,
        lang="id",
        model_type=MODEL_NAME,
        device=DEVICE,
        verbose=True,
    )
    return P.cpu().numpy().tolist(), R.cpu().numpy().tolist(), F1.cpu().numpy().tolist()

def attach_scores(data, P, R, F1):
    idx = 0
    for item in data:
        for qa in item["qa_pairs"]:
            qa["bertscore"] = {
                "precision": P[idx],
                "recall": R[idx],
                "f1": F1[idx],
            }
            idx += 1
    return data

def stats(arr):
    arr = np.array(arr, dtype=np.float64)
    return {
        "mean": float(arr.mean()),
        "min": float(arr.min()),
        "max": float(arr.max()),
        "std": float(arr.std(ddof=0)),
    }

# ===============================
# MAIN
# ===============================
def main():
    data = load_qa(INPUT_PATH)
    cands, refs = prepare_lists(data)
    P, R, F1 = compute_bertscore(cands, refs)

    # Simpan hasil QA dengan skor BERTScore
    out_json = os.path.join(OUTPUT_DIR, "voting_qa_results_with_bertscore.json")
    with open(out_json, "w", encoding="utf-8") as f:
        json.dump(attach_scores(data, P, R, F1), f, ensure_ascii=False, indent=2)

    # Simpan statistik agregat
    stats_json = os.path.join(OUTPUT_DIR, "voting_bertscore_stats.json")
    with open(stats_json, "w", encoding="utf-8") as f:
        json.dump(
            {
                "precision": stats(P),
                "recall": stats(R),
                "f1": stats(F1),
                "total_pairs": len(F1),
                "model_name": MODEL_NAME,
            },
            f,
            ensure_ascii=False,
            indent=2,
        )

    print(f"‚úÖ Skor BERTScore disimpan di: {out_json}")
    print(f"üìä Statistik BERTScore disimpan di: {stats_json}")
    print(f"Total QA pairs: {len(F1)}")

if __name__ == "__main__":
    main()

import json, os, numpy as np, torch
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm

# ===========================
# Konfigurasi & Load Model
# ===========================
model_name = "indobenchmark/indobert-base-p1"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
model.eval()

# ===========================
# Pooling untuk Dapatkan Embedding
# ===========================
def mean_pooling(last_hidden, mask):
    mask = mask.unsqueeze(-1).expand(last_hidden.size()).float()
    return (last_hidden * mask).sum(1) / torch.clamp(mask.sum(1), min=1e-9)

def encode(texts, batch_size=16):
    all_embeds = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        enc = tokenizer(batch, padding=True, truncation=True, return_tensors="pt")
        with torch.no_grad():
            out = model(**enc)
        emb = mean_pooling(out.last_hidden_state, enc['attention_mask'])
        all_embeds.append(emb)
    return torch.cat(all_embeds)

def cos_sim(a, b):
    return (a @ b) / (a.norm() * b.norm())

# ===========================
# Load QA Voting Result
# ===========================
with open("output/voting/voting_qa_result.json", encoding="utf-8") as f:
    data = json.load(f)

# ===========================
# Proses per Konteks
# ===========================
results = []
all_similarity_values = []

for item in tqdm(data, desc="Processing Contexts"):
    context = item["context"]
    qa_pairs = item["qa_pairs"]

    if len(qa_pairs) < 2:
        continue  # skip jika hanya ada 1 QA pair

    qa_texts = [f"{qa['question'].strip()} {qa['answer'].strip()}" for qa in qa_pairs]
    embeddings = encode(qa_texts)
    num_qa = len(qa_pairs)

    similarity_matrix = np.zeros((num_qa, num_qa))
    for i in range(num_qa):
        for j in range(num_qa):
            sim = cos_sim(embeddings[i], embeddings[j]).item()
            similarity_matrix[i][j] = sim
            if i != j:
                all_similarity_values.append(sim)

    results.append({
        "context": context,
        "qa_pairs": qa_pairs,
        "similarity_matrix": similarity_matrix.tolist()
    })

# ===========================
# Simpan Hasil Matrix
# ===========================
out_dir = "output/voting"
os.makedirs(out_dir, exist_ok=True)

matrix_path = os.path.join(out_dir, "cosine_qa_matrix_by_context.json")
with open(matrix_path, "w", encoding="utf-8") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

# ===========================
# Statistik Global
# ===========================
all_similarity_array = np.array(all_similarity_values)
stats = {
    "similarity_mean": float(np.mean(all_similarity_array)),
    "similarity_max": float(np.max(all_similarity_array)),
    "similarity_min": float(np.min(all_similarity_array)),
    "similarity_std": float(np.std(all_similarity_array))
}

stats_path = os.path.join(out_dir, "cosine_qa_matrix_stats.json")
with open(stats_path, "w", encoding="utf-8") as f:
    json.dump(stats, f, indent=2)

# ===========================
# Print Ringkasan
# ===========================
print("\n===== Global Cosine Similarity Stats (All Contexts) =====")
for k, v in stats.items():
    print(f"{k}: {v:.4f}")
print(f"\n‚úîÔ∏è Cosine similarity matrix saved to: {matrix_path}")
print(f"‚úîÔ∏è Stats saved to: {stats_path}")

"""# ATTENTION"""

import json, os
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Tuple, Union
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ LOAD DATA ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with open("output/QA_Pairs/bert_qa_results_sync.json", "r", encoding="utf-8") as f:
    bert_data = json.load(f)
with open("output/QA_Pairs/t5_qa_results_sync.json", "r", encoding="utf-8") as f:
    t5_data = json.load(f)
with open("output/QA_Pairs/pegasusx_qa_results_sync.json", "r", encoding="utf-8") as f:
    pegasusx_data = json.load(f)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ MODEL CLASSES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class QASelectionModel(nn.Module):
    def __init__(self, embed_dim: int, num_models: int, num_heads: int = 4, dropout: float = 0.1):
        super().__init__()
        self.model_embeddings = nn.Embedding(num_models, embed_dim)
        self.self_attn = SelfAttention(embed_dim, num_heads, dropout)
        self.cross_attn = CrossAttention(embed_dim, num_heads, dropout)
        self.weight_predictor = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 2), nn.GELU(), nn.Linear(embed_dim // 2, 1)
        )
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x: torch.Tensor, model_ids: torch.Tensor):
        x = self.norm(x + self.model_embeddings(model_ids))
        x = self.self_attn(x)
        if torch.unique(model_ids).numel() > 1:
            x = self.cross_attn(x, model_ids)
        return self.weight_predictor(x).squeeze(-1)

class SelfAttention(nn.Module):
    def __init__(self, embed_dim, heads=4, dropout=0.1):
        super().__init__()
        self.attn = nn.MultiheadAttention(embed_dim, heads, batch_first=True, dropout=dropout)
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x):
        y, _ = self.attn(x.unsqueeze(0), x.unsqueeze(0), x.unsqueeze(0))
        return self.norm(x + y.squeeze(0))

class CrossAttention(nn.Module):
    def __init__(self, embed_dim, heads=4, dropout=0.1):
        super().__init__()
        self.attn = nn.MultiheadAttention(embed_dim, heads, batch_first=True, dropout=dropout)
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x, model_ids):
        outs = []
        for mid in torch.unique(model_ids):
            q = x[model_ids == mid].unsqueeze(0)
            others = x[model_ids != mid].unsqueeze(0)
            if others.size(1) > 0:
                y, _ = self.attn(q, others, others)
                outs.append(y.squeeze(0))
            else:
                outs.append(q.squeeze(0))
        return self.norm(torch.cat(outs, dim=0))

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ PROCESSOR CLASS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class QAEmbeddingProcessor:
    def __init__(self, model_name="sentence-transformers/all-mpnet-base-v2", device="cpu"):
        self.device = torch.device(device)
        self.encoder = SentenceTransformer(model_name).to(self.device)
        self.embed_dim = self.encoder.get_sentence_embedding_dimension() * 2
        self.model_names = ["bert", "t5", "pegasusx"]

    def _encode(self, qa_list: List[Dict[str, str]]) -> torch.Tensor:
        questions = [qa["question"] for qa in qa_list]
        answers = [qa["answer"] for qa in qa_list]
        q = self.encoder.encode(questions, convert_to_tensor=True, device=self.device)
        a = self.encoder.encode(answers, convert_to_tensor=True, device=self.device)
        return torch.cat([q, a], dim=-1)

    def fuse(self, qa_lists: List[List[Dict[str, str]]], temperature=1.0) -> List[Dict[str, Union[str, float]]]:
        all_qa_pairs = []
        all_embeddings = []
        model_ids = []

        for model_index, qa_list in enumerate(qa_lists):
            if not qa_list:
                continue
            embeddings = self._encode(qa_list)
            all_embeddings.append(embeddings)
            model_ids.append(torch.full((len(qa_list),), model_index, device=self.device))
            for qa in qa_list:
                qa['model'] = self.model_names[model_index]  # Menggunakan nama model yang jelas
            all_qa_pairs.extend(qa_list)

        if not all_embeddings:
            return []

        all_embeddings = torch.cat(all_embeddings, dim=0)
        model_ids = torch.cat(model_ids, dim=0)

        model = QASelectionModel(self.embed_dim, len(qa_lists)).to(self.device).eval()
        with torch.no_grad():
            weights = F.softmax(model(all_embeddings, model_ids) / temperature, dim=-1)

        for i, qa in enumerate(all_qa_pairs):
            qa['attention_weight'] = float(weights[i].item())

        return all_qa_pairs

    def process_by_context(self, data_models: List[List[Dict]], temperature=1.0):
        num_models = len(data_models)
        context_map: Dict[Tuple[str, str], List[List[Dict]]] = {}

        for m_idx, data in enumerate(data_models):
            for item in data:
                title = item.get("title", "Unknown Title").strip()
                context = item.get("context", "Unknown").strip()
                key = (title, context)
                if key not in context_map:
                    context_map[key] = [[] for _ in range(num_models)]
                context_map[key][m_idx] = item.get("qa_pairs", [])

        results = []
        for (title, context), qa_lists in tqdm(context_map.items(), desc="Processing by context"):
            fused = self.fuse(qa_lists, temperature)
            if fused:
                results.append({
                    "title": title,
                    "context": context,
                    "qa_pairs": fused
                })
        return results

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ MAIN EXECUTION ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if __name__ == "__main__":
    processor = QAEmbeddingProcessor(device="cpu")
    all_data = [bert_data, t5_data, pegasusx_data]

    results_by_context = processor.process_by_context(all_data, temperature=1.0)

    out_dir = "output/attention"
    os.makedirs(out_dir, exist_ok=True)
    out_file = os.path.join(out_dir, "final_qa_by_context.json")

    with open(out_file, "w", encoding="utf-8") as f:
        json.dump(results_by_context, f, indent=2, ensure_ascii=False)

    print(f"‚úÖ Saved {len(results_by_context)} items to {out_file}")

# -------------- fungsi util sederhana --------------
def count_qa_pairs(data: List[Dict]) -> int:
    """Hitung total QA pairs di satu list hasil model."""
    return sum(len(item["qa_pairs"]) for item in data)

# -------------- sebelum proses attention -----------
total_bert = count_qa_pairs(bert_data)
total_t5   = count_qa_pairs(t5_data)
total_peg  = count_qa_pairs(pegasusx_data)

print("\n===== QA Pairs Summary (Input) =====")
print(f"Total QA pairs (BERT)     : {total_bert}")
print(f"Total QA pairs (T5)       : {total_t5}")
print(f"Total QA pairs (PEGASUSX) : {total_peg}")
print(f"Total QA pairs (All Input): {total_bert + total_t5 + total_peg}\n")

# -------------- setelah results_by_context dibuat --
total_final = sum(len(item["qa_pairs"]) for item in results_by_context)

print("===== QA Pairs Summary (Attention Output) =====")
print(f"Total contexts processed  : {len(results_by_context)}")
print(f"Total QA pairs (Fused)    : {total_final}\n")

# (opsional) cek konsistensi jumlah per konteks
for item in results_by_context[:5]:  # tampilkan 5 konteks pertama
    context_preview = item["context"][:60].replace("\n", " ")
    print(f"Context preview: {context_preview}... -> {len(item['qa_pairs'])} QA")


import json
from bert_score import score

# ========== Load File ==========
with open("output/attention/final_qa_by_context.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# ========== Prepare Inputs for BERTScore ==========
contexts = []
candidates = []
results = []

for item in data:
    context_text = item["context"]
    for qa in item["qa_pairs"]:
        question = qa["question"].replace("answer> ", "").strip()
        answer = qa["answer"].strip()
        full_qa = f"{question} {answer}"

        contexts.append(context_text)
        candidates.append(full_qa)

        results.append({
            "context": context_text,
            "question": question,
            "answer": answer,
            "qa_combined": full_qa
        })

# ========== Compute BERTScore ==========
P, R, F1 = score(candidates, contexts, lang="id", verbose=True)

# ========== Add Scores to Results ==========
for i in range(len(results)):
    results[i]["BERTScore_P"] = round(P[i].item(), 4)
    results[i]["BERTScore_R"] = round(R[i].item(), 4)
    results[i]["BERTScore_F1"] = round(F1[i].item(), 4)

# ========== Save as JSON ==========
with open("output/attention/attention_bertscore_results.json", "w", encoding="utf-8") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)


import json
import os
import statistics
from bert_score import score

# ========== Load File ==========
with open("output/attention/final_qa_by_context.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# ========== Prepare Inputs for BERTScore ==========
contexts = []
candidates = []

# Jika data adalah list of dict dengan key: title, context, qa_pairs
for entry in data:
    context = entry["context"]
    for qa in entry["qa_pairs"]:
        question = qa["question"].replace("answer> ", "").strip()
        answer = qa["answer"].strip()
        full_qa = f"{question} {answer}"

        contexts.append(context)
        candidates.append(full_qa)

# ========== Compute BERTScore ==========
P, R, F1 = score(candidates, contexts, lang="id", verbose=True)

# Convert tensors to floats
P_vals = [p.item() for p in P]
R_vals = [r.item() for r in R]
F1_vals = [f.item() for f in F1]

# ========== Calculate Stats ==========
def calculate_stats(values, label):
    return {
        f"{label}_mean": round(statistics.mean(values), 4),
        f"{label}_max": round(max(values), 4),
        f"{label}_min": round(min(values), 4),
        f"{label}_std": round(statistics.stdev(values), 4) if len(values) > 1 else 0.0,
    }

stats = {
    "count": len(F1_vals),
    **calculate_stats(P_vals, "P"),
    **calculate_stats(R_vals, "R"),
    **calculate_stats(F1_vals, "F1")
}

# ========== Save to JSON ==========
os.makedirs("output/attention", exist_ok=True)
with open("output/attention/attention_bertscore_stats.json", "w", encoding="utf-8") as f:
    json.dump(stats, f, indent=2, ensure_ascii=False)

print("‚úÖ Statistik BERTScore disimpan di 'output/attention/attention_bertscore_stats.json'")

import json, os, numpy as np, torch
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm

# ===========================
# Konfigurasi & Load Model
# ===========================
model_name = "indobenchmark/indobert-base-p1"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
model.eval()

# ===========================
# Pooling untuk Dapatkan Embedding
# ===========================
def mean_pooling(last_hidden, mask):
    mask = mask.unsqueeze(-1).expand(last_hidden.size()).float()
    return (last_hidden * mask).sum(1) / torch.clamp(mask.sum(1), min=1e-9)

def encode(texts, batch_size=16):
    all_embeds = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        enc = tokenizer(batch, padding=True, truncation=True, return_tensors="pt")
        with torch.no_grad():
            out = model(**enc)
        emb = mean_pooling(out.last_hidden_state, enc['attention_mask'])
        all_embeds.append(emb)
    return torch.cat(all_embeds)

def cos_sim(a, b):
    return (a @ b) / (a.norm() * b.norm())

# ===========================
# Load Data dari File
# ===========================
with open("output/attention/final_qa_by_context.json", encoding="utf-8") as f:
    data = json.load(f)

# ===========================
# Proses per Konteks
# ===========================
results = []
all_similarity_values = []

for item in tqdm(data, desc="Processing Contexts"):
    context = item["context"]
    qa_pairs = item["qa_pairs"]

    if len(qa_pairs) < 2:
        continue  # skip jika hanya ada 1 QA pair

    qa_texts = [qa["question"].replace("answer> ", "").strip() for qa in qa_pairs]
    embeddings = encode(qa_texts)
    num_qa = len(qa_texts)

    similarity_matrix = np.zeros((num_qa, num_qa))
    for i in range(num_qa):
        for j in range(num_qa):
            sim = cos_sim(embeddings[i], embeddings[j]).item()
            similarity_matrix[i][j] = sim
            if i != j:
                all_similarity_values.append(sim)

    results.append({
        "context": context,
        "qa_pairs": qa_pairs,
        "similarity_matrix": similarity_matrix.tolist()
    })

# ===========================
# Simpan Hasil Matrix
# ===========================
out_dir = "output/attention"
os.makedirs(out_dir, exist_ok=True)

matrix_path = os.path.join(out_dir, "cosine_question_matrix_by_context.json")
with open(matrix_path, "w", encoding="utf-8") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

# ===========================
# Statistik Global
# ===========================
all_similarity_array = np.array(all_similarity_values)
stats = {
    "similarity_mean": float(np.mean(all_similarity_array)),
    "similarity_max": float(np.max(all_similarity_array)),
    "similarity_min": float(np.min(all_similarity_array)),
    "similarity_std": float(np.std(all_similarity_array))
}

stats_path = os.path.join(out_dir, "cosine_question_matrix_stats.json")
with open(stats_path, "w", encoding="utf-8") as f:
    json.dump(stats, f, indent=2)

# ===========================
# Print Ringkasan
# ===========================
print("\n===== Global Cosine Similarity Stats (All Contexts) =====")
for k, v in stats.items():
    print(f"{k}: {v:.4f}")
print(f"\n‚úîÔ∏è Cosine similarity matrix saved to: {matrix_path}")
print(f"‚úîÔ∏è Stats saved to: {stats_path}")

